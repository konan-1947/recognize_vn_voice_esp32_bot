#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ASR Processor - X·ª≠ l√Ω ASR v·ªõi Google Speech Recognition + Circular Buffer
"""

import struct
import time
import tempfile
import wave
import os
import numpy as np

import audio_utils.server_config as config
from .audio_processing import audio_preprocessing_improved
from .speech_recognition import transcribe_audio_with_google
from .wake_word_handler import (
    check_wake_word, process_wake_word_detection, 
    process_question_capture, reset_question_mode
)

def save_audio_to_wav(audio_data, sample_rate=16000):
    """L∆∞u audio data th√†nh file WAV t·∫°m th·ªùi"""
    try:
        # ƒê·∫£m b·∫£o audio data c√≥ ƒë·ªô d√†i ch·∫µn (16-bit = 2 bytes)
        if len(audio_data) % 2 != 0:
            audio_data = audio_data[:-1]  # B·ªè byte cu·ªëi n·∫øu l·∫ª
        
        # T·∫°o file WAV t·∫°m th·ªùi
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
            temp_path = temp_file.name
            
        # T·∫°o WAV file v·ªõi format ch√≠nh x√°c
        with wave.open(temp_path, 'wb') as wav_file:
            wav_file.setnchannels(1)  # Mono
            wav_file.setsampwidth(2)  # 16-bit
            wav_file.setframerate(sample_rate)
            wav_file.writeframes(audio_data)
        
        print(f"‚úÖ WAV file created: {temp_path}, size: {len(audio_data)} bytes, samples: {len(audio_data)//2}")
        return temp_path
        
    except Exception as e:
        print(f"‚ùå L·ªói t·∫°o WAV file: {e}")
        return None

def asr_worker(socketio):
    """Thread x·ª≠ l√Ω ASR v·ªõi Google Speech Recognition + Circular Buffer"""
    
    print("üé§ ASR Worker ƒë√£ s·∫µn s√†ng x·ª≠ l√Ω audio v·ªõi Google Speech Recognition + Circular Buffer...")
    print(f"üìä C·∫•u h√¨nh: circular_buffer_size={config.CIRCULAR_BUFFER_SIZE}, lookback_size={config.LOOKBACK_SIZE}")
    print(f"üåê Ng√¥n ng·ªØ: {config.GOOGLE_SPEECH_LANGUAGE}")
    
    # Circular buffer v·ªõi lookback
    circular_buffer = bytearray(config.CIRCULAR_BUFFER_SIZE)
    buffer_head = 0  # V·ªã tr√≠ hi·ªán t·∫°i trong buffer
    buffer_tail = 0  # V·ªã tr√≠ b·∫Øt ƒë·∫ßu speech
    is_recording = False  # Tr·∫°ng th√°i ƒëang record
    consecutive_silence_count = 0
    
    # Adaptive threshold ƒë·ªÉ c·∫£i thi·ªán speech detection
    adaptive_rms_threshold = config.MIN_SPEECH_RMS
    recent_rms_values = []
    max_recent_rms = 1000  # Gi√° tr·ªã t·ªëi ƒëa ƒë·ªÉ tr√°nh qu√° nh·∫°y
    
    # Th√™m tracking ƒë·ªÉ tr√°nh spam API calls
    last_api_call_time = 0.0  # Th·ªùi gian g·ªçi API cu·ªëi c√πng
    
    processed_chunks = 0
    empty_queue_count = 0  # Counter ƒë·ªÉ tr√°nh spam log
    
    while not config.shutdown_event.is_set():
        try:
            # Ki·ªÉm tra queue c√≥ data kh√¥ng tr∆∞·ªõc khi pop
            if len(config.q_audio) == 0:
                empty_queue_count += 1
                if empty_queue_count % 1000 == 0:  # Log m·ªói 1000 l·∫ßn queue tr·ªëng
                    print(f"‚è≥ ASR Worker: Queue tr·ªëng, ƒëang ch·ªù audio data... (count: {empty_queue_count})")
                time.sleep(0.01)  # Sleep 10ms n·∫øu queue tr·ªëng
                continue
            
            # Reset counter khi c√≥ data
            empty_queue_count = 0
            chunk, timestamp, seq = config.q_audio.popleft()
            processed_chunks += 1
            
            # Debug logging m·ªói 100 chunks
            if processed_chunks % 100 == 0:
                print(f"üîÑ ASR Worker: ƒê√£ x·ª≠ l√Ω {processed_chunks} chunks")
            
            # Ki·ªÉm tra ti·∫øng ·ªìn (silence detection) v·ªõi circular buffer
            if len(chunk) >= 2:
                samples = struct.unpack(f"<{len(chunk)//2}h", chunk)
                rms = (sum(s*s for s in samples) / len(samples)) ** 0.5
                max_amp = max(abs(s) for s in samples)
                
                # C·∫≠p nh·∫≠t adaptive threshold
                recent_rms_values.append(rms)
                if len(recent_rms_values) > 100:  # Gi·ªØ 100 gi√° tr·ªã g·∫ßn nh·∫•t
                    recent_rms_values.pop(0)
                
                # T√≠nh threshold ƒë·ªông d·ª±a tr√™n background noise
                if len(recent_rms_values) > 20:
                    background_rms = np.mean(sorted(recent_rms_values)[:20])  # 20 gi√° tr·ªã th·∫•p nh·∫•t
                    adaptive_rms_threshold = max(config.MIN_SPEECH_RMS * 0.5, background_rms * 2)
                    adaptive_rms_threshold = min(adaptive_rms_threshold, max_recent_rms)
                
                # Logic ph√°t hi·ªán speech/silence
                is_speech = _detect_speech(rms, max_amp, samples, adaptive_rms_threshold)
                is_silence = _detect_silence(rms, max_amp, samples)
                
                # X·ª≠ l√Ω circular buffer
                buffer_head, buffer_tail, is_recording, consecutive_silence_count = _process_audio_chunk(
                    chunk, is_speech, is_silence, circular_buffer, buffer_head, buffer_tail,
                    is_recording, consecutive_silence_count, processed_chunks, rms, max_amp
                )
                
            # Ki·ªÉm tra ƒëi·ªÅu ki·ªán x·ª≠ l√Ω audio
            should_process, current_time = _should_process_audio(
                is_recording, consecutive_silence_count, buffer_head, buffer_tail, last_api_call_time
            )
            
            if should_process:
                # X·ª≠ l√Ω audio v√† nh·∫≠n d·∫°ng
                result = _process_audio_recognition(
                    circular_buffer, buffer_head, buffer_tail, 
                    timestamp, seq, socketio, current_time
                )
                
                if result:
                    last_api_call_time = current_time
                
                # Reset buffer sau khi x·ª≠ l√Ω
                circular_buffer = bytearray(config.CIRCULAR_BUFFER_SIZE)
                buffer_head = 0
                buffer_tail = 0
                is_recording = False
                consecutive_silence_count = 0
                            
        except Exception as e:
            print(f"‚ùå L·ªói ASR worker: {e}")
            reset_question_mode()
            is_recording = False
            consecutive_silence_count = 0
            continue

def _detect_speech(rms, max_amp, samples, adaptive_rms_threshold):
    """Ph√°t hi·ªán speech t·ª´ audio samples"""
    return (
        rms > adaptive_rms_threshold or 
        max_amp > config.MIN_AMPLITUDE_THRESHOLD or
        any(abs(s) > 800 for s in samples) or  # C√≥ √≠t nh·∫•t 1 sample m·∫°nh
        rms > (config.MIN_SPEECH_RMS * 0.5)  # Gi·∫£m ng∆∞·ª°ng RMS
    )

def _detect_silence(rms, max_amp, samples):
    """Ph√°t hi·ªán silence t·ª´ audio samples"""
    return (
        rms < config.SILENCE_RMS_THRESHOLD and 
        max_amp < config.SILENCE_AMPLITUDE_THRESHOLD and
        not any(abs(s) > 800 for s in samples)  # Kh√¥ng c√≥ sample m·∫°nh n√†o
    )

def _process_audio_chunk(chunk, is_speech, is_silence, circular_buffer, buffer_head, buffer_tail,
                        is_recording, consecutive_silence_count, processed_chunks, rms, max_amp):
    """X·ª≠ l√Ω chunk audio v√† c·∫≠p nh·∫≠t circular buffer"""
    
    if is_speech:
        # Reset silence counter khi c√≥ speech
        if consecutive_silence_count > 0:
            print(f"üîá Reset silence: {consecutive_silence_count} ‚Üí 0")
        consecutive_silence_count = 0
        
        # Th√™m chunk v√†o circular buffer
        chunk_size = len(chunk)
        for i in range(chunk_size):
            circular_buffer[buffer_head] = chunk[i]
            buffer_head = (buffer_head + 1) % config.CIRCULAR_BUFFER_SIZE
        
        if not is_recording:
            # B·∫Øt ƒë·∫ßu record, l√πi l·∫°i LOOKBACK_SIZE
            buffer_tail = (buffer_head - config.LOOKBACK_SIZE) % config.CIRCULAR_BUFFER_SIZE
            if buffer_tail < 0:
                buffer_tail = 0
            is_recording = True
            print(f"üé§ B·∫Øt ƒë·∫ßu record: RMS={rms:.0f}")
        else:
            # C·∫≠p nh·∫≠t tail n·∫øu c·∫ßn ƒë·ªÉ l·∫•y c√¢u d√†i h∆°n
            current_tail = (buffer_head - config.LOOKBACK_SIZE) % config.CIRCULAR_BUFFER_SIZE
            if current_tail < 0:
                current_tail = 0
            # Ch·ªâ c·∫≠p nh·∫≠t tail n·∫øu n√≥ g·∫ßn head h∆°n
            if (buffer_head - current_tail) % config.CIRCULAR_BUFFER_SIZE > (buffer_head - buffer_tail) % config.CIRCULAR_BUFFER_SIZE:
                buffer_tail = current_tail
                
    elif is_silence:
        # C√ì silence r√µ r√†ng - tƒÉng silence counter
        consecutive_silence_count += 1
        silence_duration = consecutive_silence_count * 0.02  # 20ms per chunk
        
        # Debug silence detection
        if consecutive_silence_count % 5 == 0:
            print(f"üîá Silence: {consecutive_silence_count} consecutive, RMS={rms:.0f}, Duration={silence_duration:.2f}s")
        
        # Th√™m chunk v√†o circular buffer ngay c·∫£ khi silence
        chunk_size = len(chunk)
        for i in range(chunk_size):
            circular_buffer[buffer_head] = chunk[i]
            buffer_head = (buffer_head + 1) % config.CIRCULAR_BUFFER_SIZE
    else:
        # Tr∆∞·ªùng h·ª£p kh√¥ng r√µ r√†ng - v·∫´n tƒÉng silence counter nh·∫π
        consecutive_silence_count += 1
        # Th√™m chunk v√†o circular buffer
        chunk_size = len(chunk)
        for i in range(chunk_size):
            circular_buffer[buffer_head] = chunk[i]
            buffer_head = (buffer_head + 1) % config.CIRCULAR_BUFFER_SIZE
    
    return buffer_head, buffer_tail, is_recording, consecutive_silence_count

def _should_process_audio(is_recording, consecutive_silence_count, buffer_head, buffer_tail, last_api_call_time):
    """Ki·ªÉm tra c√≥ n√™n x·ª≠ l√Ω audio kh√¥ng"""
    silence_duration = consecutive_silence_count * 0.02  # 20ms per chunk
    
    # T√≠nh th·ªùi gian recording hi·ªán t·∫°i
    if is_recording:
        if buffer_head >= buffer_tail:
            buffer_size = buffer_head - buffer_tail
        else:
            buffer_size = (config.CIRCULAR_BUFFER_SIZE - buffer_tail) + buffer_head
        current_recording_duration = buffer_size / 32000.0  # 32000 bytes = 1 gi√¢y
    else:
        current_recording_duration = 0.0
    
    # ƒêi·ªÅu ki·ªán x·ª≠ l√Ω: silence ƒë·ªß l√¢u HO·∫∂C ƒë·∫°t max duration
    should_process = (
        is_recording and (
            silence_duration >= config.MIN_SILENCE_DURATION or  # Silence ƒë·ªß l√¢u (1.0s)
            current_recording_duration >= config.MAX_RECORDING_DURATION  # ƒê·∫°t max duration
        )
    )
    
    # Th√™m ki·ªÉm tra delay ƒë·ªÉ tr√°nh spam API calls
    current_time = time.time()
    time_since_last_api = current_time - last_api_call_time
    if time_since_last_api < config.MIN_API_CALL_DELAY:
        should_process = False
    
    return should_process, current_time

def _process_audio_recognition(circular_buffer, buffer_head, buffer_tail, timestamp, seq, socketio, current_time):
    """X·ª≠ l√Ω nh·∫≠n d·∫°ng gi·ªçng n√≥i"""
    
    print(f"üéØ B·∫ÆT ƒê·∫¶U X·ª¨ L√ù AUDIO")
    
    # Tr√≠ch xu·∫•t audio t·ª´ circular buffer (t·ª´ tail ƒë·∫øn head)
    if buffer_head >= buffer_tail:
        # Buffer kh√¥ng b·ªã wrap
        audio_data = bytes(circular_buffer[buffer_tail:buffer_head])
    else:
        # Buffer b·ªã wrap, c·∫ßn n·ªëi 2 ph·∫ßn
        audio_data = bytes(circular_buffer[buffer_tail:]) + bytes(circular_buffer[:buffer_head])
    
    print(f"üéµ Audio: {len(audio_data)} bytes, duration: {len(audio_data)//32000:.1f}s")
    
    # Ki·ªÉm tra audio quality
    if len(audio_data) >= 2:
        samples = struct.unpack(f"<{len(audio_data)//2}h", audio_data)
        rms = (sum(s*s for s in samples) / len(samples)) ** 0.5
        max_amplitude = max(abs(s) for s in samples)
        duration_seconds = len(samples) / 16000.0
        
        # Ki·ªÉm tra ƒëi·ªÅu ki·ªán x·ª≠ l√Ω audio
        should_process_audio = (
            rms > (config.MIN_SPEECH_RMS * 0.5) or  # Gi·∫£m ng∆∞·ª°ng RMS
            max_amplitude > (config.MIN_AMPLITUDE_THRESHOLD * 0.5) or  # Gi·∫£m ng∆∞·ª°ng amplitude
            any(abs(s) > 600 for s in samples) or  # C√≥ √≠t nh·∫•t 1 sample m·∫°nh
            duration_seconds > 0.5  # Audio ƒë·ªß d√†i (√≠t nh·∫•t 0.5s)
        )
        
        if should_process_audio:
            print(f"‚úÖ Audio ƒë·ªß ch·∫•t l∆∞·ª£ng ƒë·ªÉ x·ª≠ l√Ω")
            
            # √Åp d·ª•ng audio preprocessing
            if config.ENABLE_PREPROCESSING:
                processed_buffer = audio_preprocessing_improved(audio_data)
            else:
                processed_buffer = audio_data
            
            # L∆∞u audio th√†nh WAV file
            wav_file = save_audio_to_wav(processed_buffer)
            if wav_file:
                try:
                    # S·ª≠ d·ª•ng Google Speech Recognition ƒë·ªÉ nh·∫≠n d·∫°ng
                    print(f"üîÑ ƒêang g·ª≠i l√™n Google Speech API...")
                    transcription = transcribe_audio_with_google(wav_file, config.GOOGLE_SPEECH_LANGUAGE)
                    
                    if transcription:
                        # X·ª≠ l√Ω transcription d·ª±a tr√™n tr·∫°ng th√°i
                        if config.is_listening_for_question:
                            # CH·∫æ ƒê·ªò NGHE C√ÇU H·ªéI
                            process_question_capture(transcription, timestamp, socketio)
                        else:
                            # CH·∫æ ƒê·ªò M·∫∂C ƒê·ªäNH
                            # Ghi transcript nh∆∞ b√¨nh th∆∞·ªùng
                            if config.transcript_logger:
                                config.transcript_logger.log_transcript_simple(transcription)
                            
                            # Ki·ªÉm tra wake word
                            if check_wake_word(transcription):
                                process_wake_word_detection(transcription, timestamp, seq, socketio)
                            
                            # G·ª≠i k·∫øt qu·∫£ final nh∆∞ b√¨nh th∆∞·ªùng
                            socketio.emit("final", {
                                "text": transcription,
                                "timestamp": timestamp,
                                "seq": seq
                            })
                            print(f"üéØ Final (Google Speech): {transcription}")
                        
                        return True
                    else:
                        print(f"üîá Google Speech kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c text")
                        # N·∫øu ƒëang nghe c√¢u h·ªèi m√† kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c
                        if config.is_listening_for_question:
                            reset_question_mode()
                        return False
                        
                finally:
                    # X√≥a file WAV t·∫°m th·ªùi
                    try:
                        os.unlink(wav_file)
                    except:
                        pass
            else:
                print(f"‚ùå Kh√¥ng th·ªÉ t·∫°o WAV file")
                return False
        else:
            print(f"üîá Audio kh√¥ng ƒë·ªß ch·∫•t l∆∞·ª£ng")
            return False
    else:
        print(f"üîá Audio data qu√° ng·∫Øn: {len(audio_data)} bytes")
        return False