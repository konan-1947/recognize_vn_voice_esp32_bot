#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ESP32 + INMP441 Real-time Streaming Server v·ªõi Google Speech Recognition + Circular Buffer
S·ª≠ d·ª•ng circular buffer v·ªõi lookback ƒë·ªÉ kh√¥ng b·ªã c·ª•t c√¢u
"""

import socket
import struct
import threading
import queue
import json
import time
import os
import tempfile
import numpy as np
import speech_recognition as sr
from flask import Flask, render_template
from flask_socketio import SocketIO, emit

# ====== UDP CONFIG ======
HOST = "0.0.0.0"
UDP_PORT = 5005
FLASK_PORT = 5000
q_audio = queue.Queue()

# ====== AUDIO PROCESSING CONFIG ======
ENABLE_PREPROCESSING = True   # B·∫≠t preprocessing ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng
SILENCE_THRESHOLD = 0.01      # TƒÉng ng∆∞·ª°ng ti·∫øng ·ªìn (0.01 = 1%)
CIRCULAR_BUFFER_SIZE = 512000 # Circular buffer size (bytes) - 16.0s
LOOKBACK_SIZE = 128000        # Lookback tr∆∞·ªõc khi ph√°t hi·ªán speech (bytes) - 4.0s
HIGH_PASS_ALPHA = 0.95        # High-pass filter coefficient
COMPRESSION_THRESHOLD = 0.3   # Dynamic range compression
COMPRESSION_RATIO = 4.0       # Compression ratio
MIN_SPEECH_RMS = 500          # Ng∆∞·ª°ng RMS t·ªëi thi·ªÉu ƒë·ªÉ coi l√† speech (nh·∫°y h∆°n)
MIN_SILENCE_DURATION = 1.0    # Th·ªùi gian im l·∫∑ng t·ªëi thi·ªÉu ƒë·ªÉ k·∫øt th√∫c c√¢u (gi√¢y)

# ====== GOOGLE SPEECH CONFIG ======
GOOGLE_SPEECH_LANGUAGE = "vi-VN"  # Ti·∫øng Vi·ªát
GOOGLE_SPEECH_TIMEOUT = 5         # Timeout 5 gi√¢y
GOOGLE_SPEECH_PHRASE_TIMEOUT = 1  # Timeout gi·ªØa c√°c t·ª´
GOOGLE_SPEECH_NON_SPEAKING_DURATION = 0.5  # Th·ªùi gian im l·∫∑ng ƒë·ªÉ k·∫øt th√∫c

# ====== Flask + SocketIO ======
app = Flask(__name__)
socketio = SocketIO(app, cors_allowed_origins="*")

@app.route('/')
def index():
    """Trang ch·ªß v·ªõi giao di·ªán real-time transcript"""
    return render_template("index.html")

@app.route('/status')
def status():
    """API tr·∫°ng th√°i server"""
    return {
        "status": "running",
        "speech_engine": "Google Speech Recognition + Circular Buffer",
        "language": GOOGLE_SPEECH_LANGUAGE,
        "udp_port": UDP_PORT,
        "flask_port": FLASK_PORT,
        "audio_queue_size": q_audio.qsize()
    }

@app.route('/test')
def test():
    """Test k·∫øt n·ªëi"""
    return {"message": "Server ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng", "timestamp": time.time()}

def audio_preprocessing(audio_data):
    """X·ª≠ l√Ω √¢m thanh n√¢ng cao ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c"""
    try:
        # Chuy·ªÉn bytes th√†nh numpy array
        samples = np.frombuffer(audio_data, dtype=np.int16)
        
        # 1. Normalize audio
        samples = samples.astype(np.float32) / 32768.0
        
        # 2. DC offset removal (lo·∫°i b·ªè offset DC)
        samples = samples - np.mean(samples)
        
        # 3. High-pass filter ƒë·ªÉ lo·∫°i b·ªè ti·∫øng ·ªìn t·∫ßn s·ªë th·∫•p
        alpha = HIGH_PASS_ALPHA
        filtered = np.zeros_like(samples)
        filtered[0] = samples[0]
        for i in range(1, len(samples)):
            filtered[i] = alpha * (filtered[i-1] + samples[i] - samples[i-1])
        
        # 4. Dynamic range compression (n√©n ƒë·ªông)
        threshold = COMPRESSION_THRESHOLD
        ratio = COMPRESSION_RATIO
        compressed = np.where(
            np.abs(filtered) > threshold,
            np.sign(filtered) * (threshold + (np.abs(filtered) - threshold) / ratio),
            filtered
        )
        
        # 5. Chuy·ªÉn v·ªÅ int16
        processed_samples = (compressed * 32767).astype(np.int16)
        
        return processed_samples.tobytes()

    except Exception as e:
        print(f"L·ªói audio preprocessing: {e}")
        return audio_data

def save_audio_to_wav(audio_data, sample_rate=16000):
    """L∆∞u audio data th√†nh file WAV t·∫°m th·ªùi"""
    try:
        import wave
        
        # ƒê·∫£m b·∫£o audio data c√≥ ƒë·ªô d√†i ch·∫µn (16-bit = 2 bytes)
        if len(audio_data) % 2 != 0:
            audio_data = audio_data[:-1]  # B·ªè byte cu·ªëi n·∫øu l·∫ª
        
        # T·∫°o file WAV t·∫°m th·ªùi
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
            temp_path = temp_file.name
            
        # T·∫°o WAV file v·ªõi format ch√≠nh x√°c
        with wave.open(temp_path, 'wb') as wav_file:
            wav_file.setnchannels(1)  # Mono
            wav_file.setsampwidth(2)  # 16-bit
            wav_file.setframerate(sample_rate)
            wav_file.writeframes(audio_data)
        
        print(f"‚úÖ WAV file created: {temp_path}, size: {len(audio_data)} bytes, samples: {len(audio_data)//2}")
        return temp_path
        
    except Exception as e:
        print(f"‚ùå L·ªói t·∫°o WAV file: {e}")
        return None

def transcribe_with_google_speech(audio_file_path):
    """S·ª≠ d·ª•ng Google Speech Recognition ƒë·ªÉ nh·∫≠n d·∫°ng gi·ªçng n√≥i"""
    try:
        # Kh·ªüi t·∫°o recognizer
        recognizer = sr.Recognizer()
        
        # C·∫•u h√¨nh parameters t·ªëi ∆∞u
        recognizer.energy_threshold = 100  # Gi·∫£m ng∆∞·ª°ng nƒÉng l∆∞·ª£ng
        recognizer.dynamic_energy_threshold = True  # T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh
        recognizer.pause_threshold = 0.8  # Gi·∫£m pause threshold
        recognizer.non_speaking_duration = 0.3  # Gi·∫£m non-speaking duration
        recognizer.phrase_threshold = 0.3  # Th√™m phrase threshold
        recognizer.operation_timeout = 10  # TƒÉng timeout
        
        # ƒê·ªçc audio file
        with sr.AudioFile(audio_file_path) as source:
            print(f"üé§ ƒêang ƒë·ªçc audio file: {audio_file_path}")
            # ƒêi·ªÅu ch·ªânh cho ambient noise
            recognizer.adjust_for_ambient_noise(source, duration=0.1)
            audio = recognizer.record(source)
        
        # Nh·∫≠n d·∫°ng v·ªõi Google Speech
        print(f"üîÑ ƒêang g·ª≠i ƒë·∫øn Google Speech API...")
        start_time = time.time()
        
        text = recognizer.recognize_google(
            audio,
            language=GOOGLE_SPEECH_LANGUAGE,
            show_all=False  # Ch·ªâ l·∫•y k·∫øt qu·∫£ t·ªët nh·∫•t
        )
        
        processing_time = time.time() - start_time
        print(f"‚úÖ Google Speech x·ª≠ l√Ω xong trong {processing_time:.2f}s")
        
        return text.strip()
        
    except sr.UnknownValueError:
        print("üîá Google Speech kh√¥ng th·ªÉ nh·∫≠n d·∫°ng ƒë∆∞·ª£c gi·ªçng n√≥i")
        print("üí° C√≥ th·ªÉ do: audio qu√° ng·∫Øn, ti·∫øng ·ªìn cao, ho·∫∑c kh√¥ng c√≥ gi·ªçng n√≥i")
        return ""
    except sr.RequestError as e:
        print(f"‚ùå L·ªói Google Speech API: {e}")
        return ""
    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω Google Speech: {e}")
        return ""

def udp_listener():
    """Thread l·∫Øng nghe UDP audio t·ª´ ESP32"""
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    sock.bind((HOST, UDP_PORT))
    print(f"üéß UDP Audio server ƒëang l·∫Øng nghe tr√™n {HOST}:{UDP_PORT}")
    print(f"üì° ƒêang ch·ªù audio packets t·ª´ ESP32...")
    
    packet_count = 0
    
    while True:
        try:
            data, addr = sock.recvfrom(2048)
            packet_count += 1
            
            if packet_count % 100 == 0:  # Log m·ªói 100 packets
                print(f"üì¶ Nh·∫≠n {packet_count} packets t·ª´ {addr}")
            
            if len(data) <= 12: 
                continue
                
            # Parse header ESP32: seq(4) + time_ms(4) + codec(1) + len24(3)
            try:
                seq, time_ms, codec, len_b2, len_b1, len_b0 = struct.unpack_from("<IIBBBB", data, 0)
                length = (len_b2 << 16) | (len_b1 << 8) | len_b0
                payload = data[12:12+length]
                
                if len(payload) == length:
                    q_audio.put((payload, time_ms, seq))
                    
            except struct.error as e:
                # N·∫øu kh√¥ng parse ƒë∆∞·ª£c header, coi nh∆∞ raw audio
                q_audio.put((data, int(time.time() * 1000), 0))
                
        except Exception as e:
            print(f"‚ùå L·ªói UDP listener: {e}")
            continue

def asr_worker():
    """Thread x·ª≠ l√Ω ASR v·ªõi Google Speech Recognition + Circular Buffer"""
    print("üé§ ASR Worker ƒë√£ s·∫µn s√†ng x·ª≠ l√Ω audio v·ªõi Google Speech Recognition + Circular Buffer...")
    print(f"üìä C·∫•u h√¨nh: circular_buffer_size={CIRCULAR_BUFFER_SIZE}, lookback_size={LOOKBACK_SIZE}")
    print(f"üåê Ng√¥n ng·ªØ: {GOOGLE_SPEECH_LANGUAGE}")
    
    # Circular buffer v·ªõi lookback
    circular_buffer = bytearray(CIRCULAR_BUFFER_SIZE)
    buffer_head = 0  # V·ªã tr√≠ hi·ªán t·∫°i trong buffer
    buffer_tail = 0  # V·ªã tr√≠ b·∫Øt ƒë·∫ßu speech
    is_recording = False  # Tr·∫°ng th√°i ƒëang record
    consecutive_silence_count = 0
    max_silence_count = 15
    
    processed_chunks = 0
    
    while True:
        try:
            chunk, timestamp, seq = q_audio.get()
            processed_chunks += 1
            
            if processed_chunks % 50 == 0:  # Log m·ªói 50 chunks
                print(f"üîÑ ASR Worker: ƒê√£ x·ª≠ l√Ω {processed_chunks} chunks, buffer size: {len(circular_buffer)} bytes")
            
            # Ki·ªÉm tra ti·∫øng ·ªìn (silence detection) v·ªõi circular buffer
            if len(chunk) >= 2:
                samples = struct.unpack(f"<{len(chunk)//2}h", chunk)
                rms = (sum(s*s for s in samples) / len(samples)) ** 0.5
                max_amp = max(abs(s) for s in samples)
                is_speech = (rms > MIN_SPEECH_RMS) or (max_amp > 2000)
                
                # Th√™m chunk v√†o circular buffer
                chunk_size = len(chunk)
                for i in range(chunk_size):
                    circular_buffer[buffer_head] = chunk[i]
                    buffer_head = (buffer_head + 1) % CIRCULAR_BUFFER_SIZE
                
                if is_speech:
                    if not is_recording:
                        # B·∫Øt ƒë·∫ßu record, l√πi l·∫°i LOOKBACK_SIZE
                        buffer_tail = (buffer_head - LOOKBACK_SIZE) % CIRCULAR_BUFFER_SIZE
                        if buffer_tail < 0:
                            buffer_tail = 0
                        is_recording = True
                        print(f"üé§ B·∫Øt ƒë·∫ßu record: RMS={rms:.1f}, Max={max_amp}, Tail={buffer_tail}, Head={buffer_head}")
                    else:
                        # ƒêang record, c·∫≠p nh·∫≠t tail n·∫øu c·∫ßn ƒë·ªÉ l·∫•y c√¢u d√†i h∆°n
                        current_tail = (buffer_head - LOOKBACK_SIZE) % CIRCULAR_BUFFER_SIZE
                        if current_tail < 0:
                            current_tail = 0
                        # Ch·ªâ c·∫≠p nh·∫≠t tail n·∫øu n√≥ g·∫ßn head h∆°n (ƒë·ªÉ l·∫•y c√¢u d√†i)
                        if (buffer_head - current_tail) % CIRCULAR_BUFFER_SIZE > (buffer_head - buffer_tail) % CIRCULAR_BUFFER_SIZE:
                            buffer_tail = current_tail
                            print(f"üé§ C·∫≠p nh·∫≠t tail: {buffer_tail} ƒë·ªÉ l·∫•y c√¢u d√†i h∆°n")
                    
                    consecutive_silence_count = 0
                else:
                    consecutive_silence_count += 1
                    if consecutive_silence_count % 50 == 0:  # Log m·ªói 50 l·∫ßn
                        print(f"üîá Silence: {consecutive_silence_count} consecutive, RMS={rms:.1f}, Max={max_amp}")
                    
                    # N·∫øu silence ƒë·ªß l√¢u v√† ƒëang record, ƒë√°nh d·∫•u ƒë·ªÉ x·ª≠ l√Ω
                    silence_duration = consecutive_silence_count * 0.02  # 20ms per chunk
                    if silence_duration >= MIN_SILENCE_DURATION and is_recording:
                        print(f"üîá Silence ƒë·ªß l√¢u ({silence_duration:.1f}s), ƒë√°nh d·∫•u x·ª≠ l√Ω...")
                        # Kh√¥ng continue, ƒë·ªÉ ti·∫øp t·ª•c x·ª≠ l√Ω ·ªü ph·∫ßn d∆∞·ªõi
                    
                    # Ch·ªâ continue n·∫øu ch∆∞a ƒë·ªß silence
                    if silence_duration < MIN_SILENCE_DURATION:
                        continue
            
            # X·ª≠ l√Ω khi c√≥ silence ƒë·ªß l√¢u v√† ƒëang record
            silence_duration = consecutive_silence_count * 0.02  # 20ms per chunk
            should_process = (is_recording and silence_duration >= MIN_SILENCE_DURATION)
            
            if should_process:
                # Tr√≠ch xu·∫•t audio t·ª´ circular buffer (t·ª´ tail ƒë·∫øn head)
                if buffer_head >= buffer_tail:
                    # Buffer kh√¥ng b·ªã wrap
                    audio_data = bytes(circular_buffer[buffer_tail:buffer_head])
                else:
                    # Buffer b·ªã wrap, c·∫ßn n·ªëi 2 ph·∫ßn
                    audio_data = bytes(circular_buffer[buffer_tail:]) + bytes(circular_buffer[:buffer_head])
                
                print(f"üéµ X·ª≠ l√Ω audio: {len(audio_data)} bytes, silence: {silence_duration:.1f}s")
                print(f"üìä Buffer info: Tail={buffer_tail}, Head={buffer_head}, Size={len(audio_data)}")
                
                # Debug: Ki·ªÉm tra audio quality
                if len(audio_data) >= 2:
                    samples = struct.unpack(f"<{len(audio_data)//2}h", audio_data)
                    rms = (sum(s*s for s in samples) / len(samples)) ** 0.5
                    max_amplitude = max(abs(s) for s in samples)
                    duration_seconds = len(samples) / 16000.0
                    print(f"üìä Audio stats: RMS={rms:.1f}, Max={max_amplitude}, Samples={len(samples)}, Duration={duration_seconds:.2f}s")
                    
                    # Ch·ªâ x·ª≠ l√Ω n·∫øu audio ƒë·ªß m·∫°nh
                    if rms > MIN_SPEECH_RMS or max_amplitude > 2000:
                        # √Åp d·ª•ng audio preprocessing
                        if ENABLE_PREPROCESSING:
                            processed_buffer = audio_preprocessing(audio_data)
                            print(f"üîß Audio preprocessing applied")
                        else:
                            processed_buffer = audio_data
                        
                        # L∆∞u audio th√†nh WAV file
                        wav_file = save_audio_to_wav(processed_buffer)
                        if wav_file:
                            try:
                                # S·ª≠ d·ª•ng Google Speech Recognition ƒë·ªÉ nh·∫≠n d·∫°ng
                                transcription = transcribe_with_google_speech(wav_file)
                                
                                if transcription:
                                    # G·ª≠i k·∫øt qu·∫£ cu·ªëi c√πng
                                    socketio.emit("final", {
                                        "text": transcription,
                                        "timestamp": timestamp,
                                        "seq": seq
                                    })
                                    print(f"üéØ Final (Google Speech): {transcription}")
                                    # Reset recording state
                                    is_recording = False
                                    consecutive_silence_count = 0
                                else:
                                    print(f"üîá Google Speech kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c text")
                                    # Reset recording state
                                    is_recording = False
                                    consecutive_silence_count = 0
                                    
                            finally:
                                # X√≥a file WAV t·∫°m th·ªùi
                                try:
                                    os.unlink(wav_file)
                                except:
                                    pass
                        else:
                            print(f"‚ùå Kh√¥ng th·ªÉ t·∫°o WAV file")
                            is_recording = False
                            consecutive_silence_count = 0
                    else:
                        print(f"üîá Audio qu√° y·∫øu (RMS={rms:.1f}), b·ªè qua")
                        is_recording = False
                        consecutive_silence_count = 0
                else:
                    print(f"üîá Audio data qu√° ng·∫Øn: {len(audio_data)} bytes")
                    is_recording = False
                    consecutive_silence_count = 0
                
        except Exception as e:
            print(f"‚ùå L·ªói ASR worker: {e}")
            is_recording = False
            consecutive_silence_count = 0
            continue

def create_templates():
    """T·∫°o th∆∞ m·ª•c templates v√† file HTML n·∫øu ch∆∞a c√≥"""
    os.makedirs("templates", exist_ok=True)
    
    html_content = """<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>üéôÔ∏è Live Speech Recognition (Circular Buffer)</title>
    <script src="https://cdn.socket.io/4.7.4/socket.io.min.js"></script>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            min-height: 100vh;
        }
        
        .container {
            max-width: 800px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.1);
            padding: 30px;
            border-radius: 20px;
            backdrop-filter: blur(10px);
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
        }
        
        h1 {
            text-align: center;
            margin-bottom: 30px;
            font-size: 2.5em;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
        }
        
        .status {
            text-align: center;
            margin-bottom: 20px;
            padding: 10px;
            border-radius: 10px;
            background: rgba(255, 255, 255, 0.2);
        }
        
        .transcript-container {
            background: rgba(0, 0, 0, 0.3);
            border-radius: 15px;
            padding: 20px;
            margin: 20px 0;
            min-height: 300px;
        }
        
        #partial {
            color: #ffd700;
            font-style: italic;
            font-size: 1.2em;
            margin-bottom: 15px;
            padding: 10px;
            background: rgba(255, 215, 0, 0.1);
            border-radius: 8px;
            border-left: 4px solid #ffd700;
        }
        
        #final {
            line-height: 1.6;
            font-size: 1.1em;
        }
        
        .word {
            display: inline-block;
            margin: 2px 4px;
            padding: 4px 8px;
            background: rgba(255, 255, 255, 0.2);
            border-radius: 6px;
            transition: all 0.3s ease;
        }
        
        .word:hover {
            background: rgba(255, 255, 255, 0.3);
            transform: translateY(-2px);
        }
        
        .stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 15px;
            margin-top: 20px;
        }
        
        .stat-item {
            background: rgba(255, 255, 255, 0.1);
            padding: 15px;
            border-radius: 10px;
            text-align: center;
        }
        
        .stat-value {
            font-size: 1.5em;
            font-weight: bold;
            color: #ffd700;
        }
        
        .stat-label {
            font-size: 0.9em;
            opacity: 0.8;
        }
        
        .connection-status {
            position: fixed;
            top: 20px;
            right: 20px;
            padding: 10px 20px;
            border-radius: 25px;
            font-weight: bold;
            transition: all 0.3s ease;
        }
        
        .connected {
            background: #4CAF50;
            color: white;
        }
        
        .disconnected {
            background: #f44336;
            color: white;
        }
        
        .model-info {
            background: rgba(0, 255, 0, 0.1);
            border: 1px solid #4CAF50;
            border-radius: 8px;
            padding: 10px;
            margin: 10px 0;
            text-align: center;
        }
        
        .performance-info {
            background: rgba(0, 150, 255, 0.1);
            border: 1px solid #0096FF;
            border-radius: 8px;
            padding: 10px;
            margin: 10px 0;
            text-align: center;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üéôÔ∏è Live Speech Recognition</h1>
        
        <div class="model-info">
            <strong>ü§ñ Engine:</strong> Google Speech Recognition + Circular Buffer - Kh√¥ng b·ªã c·ª•t c√¢u
        </div>
        
        <div class="performance-info">
            <strong>‚ö° Features:</strong> Lookback 1s, 4s buffer, Real-time processing
        </div>
        
        <div class="status">
            <div id="connection-status" class="connection-status disconnected">
                üî¥ ƒêang k·∫øt n·ªëi...
            </div>
        </div>
        
        <div class="transcript-container">
            <h3>üìù Transcript Real-time:</h3>
            <div id="partial"></div>
            <div id="final"></div>
        </div>
        
        <div class="stats">
            <div class="stat-item">
                <div class="stat-value" id="packets-received">0</div>
                <div class="stat-label">Packets nh·∫≠n</div>
            </div>
            <div class="stat-item">
                <div class="stat-value" id="final-count">0</div>
                <div class="stat-label">Final results</div>
            </div>
            <div class="stat-item">
                <div class="stat-value" id="speech-engine">Circular Buffer</div>
                <div class="stat-label">Engine</div>
            </div>
        </div>
    </div>

    <script>
        var socket = io();
        var finalCount = 0;
        var packetsReceived = 0;

        // K·∫øt n·ªëi Socket.IO
        socket.on("connect", function() {
            document.getElementById("connection-status").className = "connection-status connected";
            document.getElementById("connection-status").innerHTML = "üü¢ ƒê√£ k·∫øt n·ªëi";
        });

        socket.on("disconnect", function() {
            document.getElementById("connection-status").className = "connection-status disconnected";
            document.getElementById("connection-status").innerHTML = "üî¥ M·∫•t k·∫øt n·ªëi";
        });

        // Nh·∫≠n final results t·ª´ Google Speech
        socket.on("final", function(msg) {
            finalCount++;
            let div = document.getElementById("final");
            let timestamp = new Date(msg.timestamp).toLocaleTimeString();
            
            div.innerHTML += `<div class="word">${msg.text}</div>`;
            document.getElementById("partial").innerHTML = "";
            document.getElementById("final-count").innerText = finalCount;
            
            // Auto-scroll xu·ªëng d∆∞·ªõi
            div.scrollTop = div.scrollHeight;
        });

        // C·∫≠p nh·∫≠t stats
        setInterval(function() {
            // C√≥ th·ªÉ th√™m c·∫≠p nh·∫≠t stats real-time ·ªü ƒë√¢y
        }, 1000);
    </script>
</body>
</html>"""
    
    with open("templates/index.html", "w", encoding="utf-8") as f:
        f.write(html_content)
    
    print("‚úÖ Templates ƒë√£ ƒë∆∞·ª£c t·∫°o")

def check_dependencies():
    """Ki·ªÉm tra dependencies c·∫ßn thi·∫øt"""
    try:
        import speech_recognition
        print("‚úÖ SpeechRecognition ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t")
        return True
    except ImportError:
        print("‚ùå SpeechRecognition ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
        print("üì¶ C√†i ƒë·∫∑t: pip install SpeechRecognition")
        return False

if __name__ == "__main__":
    print("üöÄ Kh·ªüi ƒë·ªông ESP32 + INMP441 Real-time Streaming Server v·ªõi Circular Buffer...")
    print(f"üì° Flask server: http://localhost:{FLASK_PORT}")
    print(f"üéß UDP server: {HOST}:{UDP_PORT}")
    print(f"üåê Speech engine: Google Speech Recognition + Circular Buffer")
    print(f"üìä Buffer: {CIRCULAR_BUFFER_SIZE//1000}KB, Lookback: {LOOKBACK_SIZE//1000}KB")
    print("=" * 50)
    
    # Ki·ªÉm tra dependencies
    if not check_dependencies():
        print("‚ùå Thi·∫øu dependencies, vui l√≤ng c√†i ƒë·∫∑t tr∆∞·ªõc")
        exit(1)
    
    # T·∫°o templates
    create_templates()
    
    # Kh·ªüi ƒë·ªông c√°c threads
    udp_thread = threading.Thread(target=udp_listener, daemon=True)
    asr_thread = threading.Thread(target=asr_worker, daemon=True)
    
    print("üîÑ ƒêang kh·ªüi ƒë·ªông UDP Listener thread...")
    udp_thread.start()
    print("‚úÖ UDP Listener thread ƒë√£ kh·ªüi ƒë·ªông")
    
    print("üîÑ ƒêang kh·ªüi ƒë·ªông ASR Worker thread...")
    asr_thread.start()
    print("‚úÖ ASR Worker thread ƒë√£ kh·ªüi ƒë·ªông v·ªõi Circular Buffer")
    
    # Ki·ªÉm tra thread c√≥ alive kh√¥ng
    time.sleep(2)
    if asr_thread.is_alive():
        print("‚úÖ ASR Worker thread ƒëang ch·∫°y b√¨nh th∆∞·ªùng")
    else:
        print("‚ùå ASR Worker thread ƒë√£ d·ª´ng!")
    
    # Ch·∫°y Flask-SocketIO server
    print("üöÄ Kh·ªüi ƒë·ªông Flask-SocketIO server...")
    socketio.run(app, host="0.0.0.0", port=FLASK_PORT, debug=False) 