#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Google Speech Circular Server - Real-time speech recognition v·ªõi ESP32 + INMP441
S·ª≠ d·ª•ng circular buffer ƒë·ªÉ tr√°nh c·∫Øt c√¢u v√† Google Speech API
"""

import socket
import struct
import threading
import time
import os
import tempfile
import wave
import signal
import sys
from collections import deque
import numpy as np
from flask import Flask, render_template
from flask_socketio import SocketIO, emit

# Import t·ª´ audio_utils package
from audio_utils import (
    audio_preprocessing_improved,
    transcribe_audio_with_google,
    check_audio_dependencies,
    TranscriptLogger
)

# ====== UDP CONFIG ======
HOST = "0.0.0.0"
UDP_PORT = 5005
FLASK_PORT = 5000
q_audio = deque(maxlen=1000) # Changed from queue.Queue() to deque(maxlen=1000)

# Global flag ƒë·ªÉ d·ª´ng threads
shutdown_event = threading.Event()

def signal_handler(signum, frame):
    """Signal handler ƒë·ªÉ graceful shutdown"""
    print(f"\nüõë Nh·∫≠n signal {signum}, ƒëang d·ª´ng server...")
    shutdown_event.set()
    time.sleep(2)  # ƒê·ª£i threads d·ª´ng
    print("‚úÖ Server ƒë√£ d·ª´ng an to√†n")
    sys.exit(0)

# ƒêƒÉng k√Ω signal handlers
signal.signal(signal.SIGINT, signal_handler)   # Ctrl+C
signal.signal(signal.SIGTERM, signal_handler)  # Termination signal

# ====== AUDIO PROCESSING CONFIG ======
ENABLE_PREPROCESSING = True   # B·∫≠t preprocessing ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng
SILENCE_THRESHOLD = 0.01      # TƒÉng ng∆∞·ª°ng ti·∫øng ·ªìn (0.01 = 1%)
CIRCULAR_BUFFER_SIZE = 512000 # Circular buffer size (bytes) - 16.0s
LOOKBACK_SIZE = 128000        # Lookback tr∆∞·ªõc khi ph√°t hi·ªán speech (bytes) - 4.0s
HIGH_PASS_ALPHA = 0.95        # High-pass filter coefficient
COMPRESSION_THRESHOLD = 0.3   # Dynamic range compression
COMPRESSION_RATIO = 4.0       # Compression ratio
# ƒêi·ªÅu ch·ªânh c√°c ng∆∞·ª°ng ƒë·ªÉ nh·∫°y h∆°n:
MIN_SPEECH_RMS = 200          # Gi·∫£m t·ª´ 500 xu·ªëng 200 (nh·∫°y h∆°n)
MIN_SILENCE_DURATION = 0.5    # Gi·∫£m t·ª´ 1.0s xu·ªëng 0.5s
MIN_AMPLITUDE_THRESHOLD = 1000  # Th√™m ng∆∞·ª°ng amplitude m·ªõi
# Th√™m ng∆∞·ª°ng silence detection:
SILENCE_RMS_THRESHOLD = 300   # Ng∆∞·ª°ng RMS ƒë·ªÉ coi l√† silence
SILENCE_AMPLITUDE_THRESHOLD = 800  # Ng∆∞·ª°ng amplitude ƒë·ªÉ coi l√† silence
# Th√™m max recording duration:
MAX_RECORDING_DURATION = 10.0  # T·ªëi ƒëa 10 gi√¢y recording
MIN_API_CALL_DELAY = 1.0      # Delay t·ªëi thi·ªÉu gi·ªØa c√°c l·∫ßn g·ªçi API (gi√¢y)

# ====== GOOGLE SPEECH CONFIG ======
GOOGLE_SPEECH_LANGUAGE = "vi-VN"  # Ti·∫øng Vi·ªát
GOOGLE_SPEECH_TIMEOUT = 5         # Timeout 5 gi√¢y
GOOGLE_SPEECH_PHRASE_TIMEOUT = 1  # Timeout gi·ªØa c√°c t·ª´
GOOGLE_SPEECH_NON_SPEAKING_DURATION = 0.5  # Th·ªùi gian im l·∫∑ng ƒë·ªÉ k·∫øt th√∫c

# ====== Flask + SocketIO ======
app = Flask(__name__)
socketio = SocketIO(app, cors_allowed_origins="*")

@app.route('/')
def index():
    """Trang ch·ªß v·ªõi giao di·ªán real-time transcript"""
    return render_template("index.html")

@app.route('/status')
def status():
    """API tr·∫°ng th√°i server"""
    return {
        "status": "running",
        "speech_engine": "Google Speech Recognition + Circular Buffer",
        "language": GOOGLE_SPEECH_LANGUAGE,
        "udp_port": UDP_PORT,
        "flask_port": FLASK_PORT,
        "audio_queue_size": len(q_audio)
    }

@app.route('/test')
def test():
    """Test k·∫øt n·ªëi"""
    return {"message": "Server ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng", "timestamp": time.time()}

@app.route('/transcript-stats')
def transcript_stats():
    """API th·ªëng k√™ transcript log"""
    try:
        stats = transcript_logger.get_stats()
        return {
            "transcript_log": {
                "filepath": transcript_logger.get_filepath(),
                "stats": stats
            }
        }
    except Exception as e:
        return {"error": str(e)}

def save_audio_to_wav(audio_data, sample_rate=16000):
    """L∆∞u audio data th√†nh file WAV t·∫°m th·ªùi"""
    try:
        # ƒê·∫£m b·∫£o audio data c√≥ ƒë·ªô d√†i ch·∫µn (16-bit = 2 bytes)
        if len(audio_data) % 2 != 0:
            audio_data = audio_data[:-1]  # B·ªè byte cu·ªëi n·∫øu l·∫ª
        
        # T·∫°o file WAV t·∫°m th·ªùi
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
            temp_path = temp_file.name
            
        # T·∫°o WAV file v·ªõi format ch√≠nh x√°c
        with wave.open(temp_path, 'wb') as wav_file:
            wav_file.setnchannels(1)  # Mono
            wav_file.setsampwidth(2)  # 16-bit
            wav_file.setframerate(sample_rate)
            wav_file.writeframes(audio_data)
        
        print(f"‚úÖ WAV file created: {temp_path}, size: {len(audio_data)} bytes, samples: {len(audio_data)//2}")
        return temp_path
        
    except Exception as e:
        print(f"‚ùå L·ªói t·∫°o WAV file: {e}")
        return None

def udp_listener():
    """Thread l·∫Øng nghe UDP audio t·ª´ ESP32"""
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    sock.bind((HOST, UDP_PORT))
    print(f"üéß UDP Audio server ƒëang l·∫Øng nghe tr√™n {HOST}:{UDP_PORT}")
    print(f"üì° ƒêang ch·ªù audio packets t·ª´ ESP32...")
    
    packet_count = 0
    
    while not shutdown_event.is_set():
        try:
            # Set timeout ƒë·ªÉ c√≥ th·ªÉ ki·ªÉm tra shutdown event
            sock.settimeout(1.0)
            data, addr = sock.recvfrom(2048)
            packet_count += 1
            
            if packet_count % 500 == 0:  # Log m·ªói 500 packets
                print(f"üì¶ Nh·∫≠n {packet_count} packets t·ª´ {addr}")
            
            if len(data) <= 12: 
                continue
                
            # Parse header ESP32: seq(4) + time_ms(4) + codec(1) + len24(3)
            try:
                seq, time_ms, codec, len_b2, len_b1, len_b0 = struct.unpack_from("<IIBBBB", data, 0)
                length = (len_b2 << 16) | (len_b1 << 8) | len_b0
                payload = data[12:12+length]
                
                if len(payload) == length:
                    q_audio.append((payload, time_ms, seq))
                    
            except struct.error as e:
                # N·∫øu kh√¥ng parse ƒë∆∞·ª£c header, coi nh∆∞ raw audio
                q_audio.append((data, int(time.time() * 1000), 0))
                
        except socket.timeout:
            # Timeout, ki·ªÉm tra shutdown event
            continue
        except Exception as e:
            if not shutdown_event.is_set():
                print(f"‚ùå L·ªói UDP listener: {e}")
            continue
    
    print("üõë UDP Listener ƒë√£ d·ª´ng")
    sock.close()

def asr_worker():
    """Thread x·ª≠ l√Ω ASR v·ªõi Google Speech Recognition + Circular Buffer"""
    print("üé§ ASR Worker ƒë√£ s·∫µn s√†ng x·ª≠ l√Ω audio v·ªõi Google Speech Recognition + Circular Buffer...")
    print(f"üìä C·∫•u h√¨nh: circular_buffer_size={CIRCULAR_BUFFER_SIZE}, lookback_size={LOOKBACK_SIZE}")
    print(f"üåê Ng√¥n ng·ªØ: {GOOGLE_SPEECH_LANGUAGE}")
    
    # Circular buffer v·ªõi lookback
    circular_buffer = bytearray(CIRCULAR_BUFFER_SIZE)
    buffer_head = 0  # V·ªã tr√≠ hi·ªán t·∫°i trong buffer
    buffer_tail = 0  # V·ªã tr√≠ b·∫Øt ƒë·∫ßu speech
    is_recording = False  # Tr·∫°ng th√°i ƒëang record
    consecutive_silence_count = 0
    max_silence_count = 15
    
    # Adaptive threshold ƒë·ªÉ c·∫£i thi·ªán speech detection
    adaptive_rms_threshold = MIN_SPEECH_RMS
    recent_rms_values = []
    max_recent_rms = 1000  # Gi√° tr·ªã t·ªëi ƒëa ƒë·ªÉ tr√°nh qu√° nh·∫°y
    
    # Th√™m tracking ƒë·ªÉ tr√°nh spam API calls
    last_api_call_time = 0.0  # Th·ªùi gian g·ªçi API cu·ªëi c√πng
    
    processed_chunks = 0
    empty_queue_count = 0  # Counter ƒë·ªÉ tr√°nh spam log
    
    while not shutdown_event.is_set():
        try:
            # Ki·ªÉm tra queue c√≥ data kh√¥ng tr∆∞·ªõc khi pop
            if len(q_audio) == 0:
                empty_queue_count += 1
                if empty_queue_count % 1000 == 0:  # Log m·ªói 1000 l·∫ßn queue tr·ªëng
                    print(f"‚è≥ ASR Worker: Queue tr·ªëng, ƒëang ch·ªù audio data... (count: {empty_queue_count})")
                time.sleep(0.01)  # Sleep 10ms n·∫øu queue tr·ªëng
                continue
            
            # Reset counter khi c√≥ data
            empty_queue_count = 0
            chunk, timestamp, seq = q_audio.popleft()
            processed_chunks += 1
            
            # Debug logging m·ªói 100 chunks
            if processed_chunks % 100 == 0:
                print(f"üîÑ ASR Worker: ƒê√£ x·ª≠ l√Ω {processed_chunks} chunks")
            
            # Ki·ªÉm tra ti·∫øng ·ªìn (silence detection) v·ªõi circular buffer
            if len(chunk) >= 2:
                samples = struct.unpack(f"<{len(chunk)//2}h", chunk)
                rms = (sum(s*s for s in samples) / len(samples)) ** 0.5
                max_amp = max(abs(s) for s in samples)
                
                # C·∫≠p nh·∫≠t adaptive threshold
                recent_rms_values.append(rms)
                if len(recent_rms_values) > 100:  # Gi·ªØ 100 gi√° tr·ªã g·∫ßn nh·∫•t
                    recent_rms_values.pop(0)
                
                # T√≠nh threshold ƒë·ªông d·ª±a tr√™n background noise
                if len(recent_rms_values) > 20:
                    background_rms = np.mean(sorted(recent_rms_values)[:20])  # 20 gi√° tr·ªã th·∫•p nh·∫•t
                    adaptive_rms_threshold = max(MIN_SPEECH_RMS * 0.5, background_rms * 2)
                    adaptive_rms_threshold = min(adaptive_rms_threshold, max_recent_rms)
                
                # C·∫£i thi·ªán logic ph√°t hi·ªán speech - th√™m ng∆∞·ª°ng silence
                is_speech = (
                    rms > adaptive_rms_threshold or 
                    max_amp > MIN_AMPLITUDE_THRESHOLD or
                    any(abs(s) > 800 for s in samples) or  # C√≥ √≠t nh·∫•t 1 sample m·∫°nh
                    rms > (MIN_SPEECH_RMS * 0.5)  # Gi·∫£m ng∆∞·ª°ng RMS
                )
                
                # C·∫£i thi·ªán silence detection - th√™m ƒëi·ªÅu ki·ªán r√µ r√†ng
                is_silence = (
                    rms < SILENCE_RMS_THRESHOLD and 
                    max_amp < SILENCE_AMPLITUDE_THRESHOLD and
                    not any(abs(s) > 800 for s in samples)  # Kh√¥ng c√≥ sample m·∫°nh n√†o
                )
                
                # Debug logging t·∫°m th·ªùi ƒë·ªÉ diagnose
                if processed_chunks % 50 == 0:  # Log m·ªói 50 chunks
                    print(f"üîç Debug: RMS={rms:.0f}, Max={max_amp}, Speech={is_speech}, Silence={is_silence}, Threshold={adaptive_rms_threshold:.0f}")
                    print(f"üîç Silence counters: consecutive={consecutive_silence_count}, duration={consecutive_silence_count * 0.02:.2f}s")
                
                if is_speech:
                    # Ch·ªâ log khi b·∫Øt ƒë·∫ßu record m·ªõi
                    if not is_recording:
                        print(f"üé§ B·∫Øt ƒë·∫ßu record: RMS={rms:.0f}")
                    
                    # Reset silence counter khi c√≥ speech
                    if consecutive_silence_count > 0:
                        print(f"üîá Reset silence: {consecutive_silence_count} ‚Üí 0")
                    consecutive_silence_count = 0
                    
                    # Th√™m chunk v√†o circular buffer
                    chunk_size = len(chunk)
                    for i in range(chunk_size):
                        circular_buffer[buffer_head] = chunk[i]
                        buffer_head = (buffer_head + 1) % CIRCULAR_BUFFER_SIZE
                    
                    if not is_recording:
                        # B·∫Øt ƒë·∫ßu record, l√πi l·∫°i LOOKBACK_SIZE
                        buffer_tail = (buffer_head - LOOKBACK_SIZE) % CIRCULAR_BUFFER_SIZE
                        if buffer_tail < 0:
                            buffer_tail = 0
                        is_recording = True
                    else:
                        # ƒêang record, c·∫≠p nh·∫≠t tail n·∫øu c·∫ßn ƒë·ªÉ l·∫•y c√¢u d√†i h∆°n
                        current_tail = (buffer_head - LOOKBACK_SIZE) % CIRCULAR_BUFFER_SIZE
                        if current_tail < 0:
                            current_tail = 0
                        # Ch·ªâ c·∫≠p nh·∫≠t tail n·∫øu n√≥ g·∫ßn head h∆°n (ƒë·ªÉ l·∫•y c√¢u d√†i)
                        if (buffer_head - current_tail) % CIRCULAR_BUFFER_SIZE > (buffer_head - buffer_tail) % CIRCULAR_BUFFER_SIZE:
                            buffer_tail = current_tail
                        
                        # Log recording duration m·ªói 100 chunks
                        if processed_chunks % 100 == 0:
                            buffer_size = (buffer_head - buffer_tail) % CIRCULAR_BUFFER_SIZE
                            recording_duration = buffer_size / 32000.0
                            print(f"üé§ ƒêang record: {recording_duration:.1f}s / {MAX_RECORDING_DURATION}s")
                elif is_silence:
                    # C√ì silence r√µ r√†ng - tƒÉng silence counter
                    consecutive_silence_count += 1
                    silence_duration = consecutive_silence_count * 0.02  # 20ms per chunk
                    
                    # Debug silence detection
                    if consecutive_silence_count % 5 == 0:  # Log m·ªói 5 l·∫ßn silence
                        print(f"üîá Silence: {consecutive_silence_count} consecutive, RMS={rms:.0f}, Duration={silence_duration:.2f}s")
                    
                    # Th√™m chunk v√†o circular buffer ngay c·∫£ khi silence
                    chunk_size = len(chunk)
                    for i in range(chunk_size):
                        circular_buffer[buffer_head] = chunk[i]
                        buffer_head = (buffer_head + 1) % CIRCULAR_BUFFER_SIZE
                    
                    # N·∫øu silence ƒë·ªß l√¢u v√† ƒëang record, ƒë√°nh d·∫•u ƒë·ªÉ x·ª≠ l√Ω
                    if silence_duration >= MIN_SILENCE_DURATION and is_recording:
                        print(f"üîá Silence ƒë·ªß l√¢u ({silence_duration:.1f}s), chu·∫©n b·ªã x·ª≠ l√Ω...")
                        # Kh√¥ng continue, ƒë·ªÉ ti·∫øp t·ª•c x·ª≠ l√Ω ·ªü ph·∫ßn d∆∞·ªõi
                    
                    # Ch·ªâ continue n·∫øu ch∆∞a ƒë·ªß silence
                    if silence_duration < MIN_SILENCE_DURATION:
                        continue
                else:
                    # Tr∆∞·ªùng h·ª£p kh√¥ng r√µ r√†ng - v·∫´n tƒÉng silence counter nh·∫π
                    consecutive_silence_count += 1
                    silence_duration = consecutive_silence_count * 0.02
                    
                    # Debug ambiguous case
                    if consecutive_silence_count % 10 == 0:
                        print(f"üîá Ambiguous: {consecutive_silence_count} consecutive, RMS={rms:.0f}, Duration={silence_duration:.2f}s")
                    
                    # Th√™m chunk v√†o circular buffer
                    chunk_size = len(chunk)
                    for i in range(chunk_size):
                        circular_buffer[buffer_head] = chunk[i]
                        buffer_head = (buffer_head + 1) % CIRCULAR_BUFFER_SIZE
                    
                    if silence_duration >= MIN_SILENCE_DURATION and is_recording:
                        print(f"üîá Ambiguous silence ƒë·ªß l√¢u ({silence_duration:.1f}s), chu·∫©n b·ªã x·ª≠ l√Ω...")
                    elif silence_duration < MIN_SILENCE_DURATION:
                        continue
                        
            # X·ª≠ l√Ω khi c√≥ silence ƒë·ªß l√¢u v√† ƒëang record HO·∫∂C ƒë·∫°t max duration
            silence_duration = consecutive_silence_count * 0.02  # 20ms per chunk
            
            # T√≠nh th·ªùi gian recording hi·ªán t·∫°i
            if is_recording:
                # T√≠nh buffer size hi·ªán t·∫°i
                if buffer_head >= buffer_tail:
                    buffer_size = buffer_head - buffer_tail
                else:
                    buffer_size = (CIRCULAR_BUFFER_SIZE - buffer_tail) + buffer_head
                
                # Chuy·ªÉn bytes th√†nh th·ªùi gian (16-bit samples, 16kHz)
                current_recording_duration = buffer_size / 32000.0  # 32000 bytes = 1 gi√¢y
            else:
                current_recording_duration = 0.0
            
            # ƒêi·ªÅu ki·ªán x·ª≠ l√Ω: silence ƒë·ªß l√¢u HO·∫∂C ƒë·∫°t max duration
            should_process = (
                is_recording and (
                    silence_duration >= MIN_SILENCE_DURATION or  # Silence ƒë·ªß l√¢u
                    current_recording_duration >= MAX_RECORDING_DURATION  # ƒê·∫°t max duration
                )
            )
            
            # Th√™m ki·ªÉm tra delay ƒë·ªÉ tr√°nh spam API calls
            current_time = time.time()
            time_since_last_api = current_time - last_api_call_time
            if time_since_last_api < MIN_API_CALL_DELAY:
                should_process = False
            
            # Debug processing conditions
            if processed_chunks % 50 == 0:
                print(f"üîç Processing check: recording={is_recording}, silence={silence_duration:.2f}s, duration={current_recording_duration:.2f}s, should_process={should_process}")
                if not should_process and time_since_last_api < MIN_API_CALL_DELAY:
                    print(f"‚è∞ API delay: {time_since_last_api:.1f}s < {MIN_API_CALL_DELAY}s")
            
            if should_process:
                if silence_duration >= MIN_SILENCE_DURATION:
                    print(f"üéØ B·∫ÆT ƒê·∫¶U X·ª¨ L√ù AUDIO: silence={silence_duration:.1f}s")
                else:
                    print(f"üéØ B·∫ÆT ƒê·∫¶U X·ª¨ L√ù AUDIO: ƒë·∫°t max duration={current_recording_duration:.1f}s")
                
                # C·∫≠p nh·∫≠t th·ªùi gian g·ªçi API
                last_api_call_time = current_time
                
                # Tr√≠ch xu·∫•t audio t·ª´ circular buffer (t·ª´ tail ƒë·∫øn head)
                if buffer_head >= buffer_tail:
                    # Buffer kh√¥ng b·ªã wrap
                    audio_data = bytes(circular_buffer[buffer_tail:buffer_head])
                else:
                    # Buffer b·ªã wrap, c·∫ßn n·ªëi 2 ph·∫ßn
                    audio_data = bytes(circular_buffer[buffer_tail:]) + bytes(circular_buffer[:buffer_head])
                
                print(f"üéµ Audio: {len(audio_data)} bytes, duration: {len(audio_data)//32000:.1f}s")
                
                # Debug: Ki·ªÉm tra audio quality
                if len(audio_data) >= 2:
                    samples = struct.unpack(f"<{len(audio_data)//2}h", audio_data)
                    rms = (sum(s*s for s in samples) / len(samples)) ** 0.5
                    max_amplitude = max(abs(s) for s in samples)
                    duration_seconds = len(samples) / 16000.0
                    
                    # C·∫£i thi·ªán ƒëi·ªÅu ki·ªán x·ª≠ l√Ω audio - nh·∫°y h∆°n
                    should_process_audio = (
                        rms > (MIN_SPEECH_RMS * 0.5) or  # Gi·∫£m ng∆∞·ª°ng RMS
                        max_amplitude > (MIN_AMPLITUDE_THRESHOLD * 0.5) or  # Gi·∫£m ng∆∞·ª°ng amplitude
                        any(abs(s) > 600 for s in samples) or  # C√≥ √≠t nh·∫•t 1 sample m·∫°nh
                        duration_seconds > 0.5  # Audio ƒë·ªß d√†i (√≠t nh·∫•t 0.5s)
                    )
                    
                    if should_process_audio:
                        print(f"‚úÖ Audio ƒë·ªß ch·∫•t l∆∞·ª£ng ƒë·ªÉ x·ª≠ l√Ω")
                        
                        # √Åp d·ª•ng audio preprocessing
                        if ENABLE_PREPROCESSING:
                            processed_buffer = audio_preprocessing_improved(audio_data) # Changed from audio_preprocessing to audio_preprocessing_improved
                        
                        # L∆∞u audio th√†nh WAV file
                        wav_file = save_audio_to_wav(processed_buffer)
                        if wav_file:
                            try:
                                # S·ª≠ d·ª•ng Google Speech Recognition ƒë·ªÉ nh·∫≠n d·∫°ng
                                print(f"üîÑ ƒêang g·ª≠i l√™n Google Speech API...")
                                transcription = transcribe_audio_with_google(wav_file, GOOGLE_SPEECH_LANGUAGE)
                                
                                if transcription:
                                    # Ghi transcript ra file log
                                    transcript_logger.log_transcript_simple(transcription)
                                    
                                    # G·ª≠i k·∫øt qu·∫£ cu·ªëi c√πng
                                    socketio.emit("final", {
                                        "text": transcription,
                                        "timestamp": timestamp,
                                        "seq": seq
                                    })
                                    print(f"üéØ Final (Google Speech): {transcription}")
                                    
                                    # X√ìA BUFFER sau khi x·ª≠ l√Ω th√†nh c√¥ng ƒë·ªÉ tr√°nh l·∫∑p l·∫°i
                                    print("üßπ ƒêang x√≥a buffer ƒë·ªÉ tr√°nh duplicate...")
                                    circular_buffer = bytearray(CIRCULAR_BUFFER_SIZE)
                                    buffer_head = 0
                                    buffer_tail = 0
                                    
                                    # Reset recording state
                                    is_recording = False
                                    consecutive_silence_count = 0
                                else:
                                    print(f"üîá Google Speech kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c text")
                                    
                                    # X√ìA BUFFER ngay c·∫£ khi kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c ƒë·ªÉ tr√°nh l·∫∑p l·∫°i
                                    print("üßπ ƒêang x√≥a buffer (kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c)...")
                                    circular_buffer = bytearray(CIRCULAR_BUFFER_SIZE)
                                    buffer_head = 0
                                    buffer_tail = 0
                                    
                                    # Reset recording state
                                    is_recording = False
                                    consecutive_silence_count = 0
                                    
                            finally:
                                # X√≥a file WAV t·∫°m th·ªùi
                                try:
                                    os.unlink(wav_file)
                                except:
                                    pass
                        else:
                            print(f"‚ùå Kh√¥ng th·ªÉ t·∫°o WAV file")
                            
                            # X√ìA BUFFER khi kh√¥ng th·ªÉ t·∫°o WAV file
                            print("üßπ ƒêang x√≥a buffer (kh√¥ng th·ªÉ t·∫°o WAV)...")
                            circular_buffer = bytearray(CIRCULAR_BUFFER_SIZE)
                            buffer_head = 0
                            buffer_tail = 0
                            
                            is_recording = False
                            consecutive_silence_count = 0
                    else:
                        print(f"üîá Audio kh√¥ng ƒë·ªß ch·∫•t l∆∞·ª£ng")
                        
                        # X√ìA BUFFER khi audio kh√¥ng ƒë·ªß ch·∫•t l∆∞·ª£ng
                        print("üßπ ƒêang x√≥a buffer (audio kh√¥ng ƒë·ªß ch·∫•t l∆∞·ª£ng)...")
                        circular_buffer = bytearray(CIRCULAR_BUFFER_SIZE)
                        buffer_head = 0
                        buffer_tail = 0
                        
                        is_recording = False
                        consecutive_silence_count = 0
                else:
                    print(f"üîá Audio data qu√° ng·∫Øn: {len(audio_data)} bytes")
                    
                    # X√ìA BUFFER khi audio qu√° ng·∫Øn
                    print("üßπ ƒêang x√≥a buffer (audio qu√° ng·∫Øn)...")
                    circular_buffer = bytearray(CIRCULAR_BUFFER_SIZE)
                    buffer_head = 0
                    buffer_tail = 0
                    
                    is_recording = False
                    consecutive_silence_count = 0
                            
        except Exception as e:
            print(f"‚ùå L·ªói ASR worker: {e}")
            is_recording = False
            consecutive_silence_count = 0
            continue

def create_templates():
    """T·∫°o th∆∞ m·ª•c templates v√† file HTML n·∫øu ch∆∞a c√≥"""
    os.makedirs("templates", exist_ok=True)
    
    html_content = """<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>üéôÔ∏è Live Speech Recognition (Circular Buffer)</title>
    <script src="https://cdn.socket.io/4.7.4/socket.io.min.js"></script>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            min-height: 100vh;
        }
        
        .container {
            max-width: 800px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.1);
            padding: 30px;
            border-radius: 20px;
            backdrop-filter: blur(10px);
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
        }
        
        h1 {
            text-align: center;
            margin-bottom: 30px;
            font-size: 2.5em;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
        }
        
        .status {
            text-align: center;
            margin-bottom: 20px;
            padding: 10px;
            border-radius: 10px;
            background: rgba(255, 255, 255, 0.2);
        }
        
        .transcript-container {
            background: rgba(0, 0, 0, 0.3);
            border-radius: 15px;
            padding: 20px;
            margin: 20px 0;
            min-height: 300px;
        }
        
        #partial {
            color: #ffd700;
            font-style: italic;
            font-size: 1.2em;
            margin-bottom: 15px;
            padding: 10px;
            background: rgba(255, 215, 0, 0.1);
            border-radius: 8px;
            border-left: 4px solid #ffd700;
        }
        
        #final {
            line-height: 1.6;
            font-size: 1.1em;
        }
        
        .word {
            display: inline-block;
            margin: 2px 4px;
            padding: 4px 8px;
            background: rgba(255, 255, 255, 0.2);
            border-radius: 6px;
            transition: all 0.3s ease;
        }
        
        .word:hover {
            background: rgba(255, 255, 255, 0.3);
            transform: translateY(-2px);
        }
        
        .stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 15px;
            margin-top: 20px;
        }
        
        .stat-item {
            background: rgba(255, 255, 255, 0.1);
            padding: 15px;
            border-radius: 10px;
            text-align: center;
        }
        
        .stat-value {
            font-size: 1.5em;
            font-weight: bold;
            color: #ffd700;
        }
        
        .stat-label {
            font-size: 0.9em;
            opacity: 0.8;
        }
        
        .connection-status {
            position: fixed;
            top: 20px;
            right: 20px;
            padding: 10px 20px;
            border-radius: 25px;
            font-weight: bold;
            transition: all 0.3s ease;
        }
        
        .connected {
            background: #4CAF50;
            color: white;
        }
        
        .disconnected {
            background: #f44336;
            color: white;
        }
        
        .model-info {
            background: rgba(0, 255, 0, 0.1);
            border: 1px solid #4CAF50;
            border-radius: 8px;
            padding: 10px;
            margin: 10px 0;
            text-align: center;
        }
        
        .performance-info {
            background: rgba(0, 150, 255, 0.1);
            border: 1px solid #0096FF;
            border-radius: 8px;
            padding: 10px;
            margin: 10px 0;
            text-align: center;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üéôÔ∏è Live Speech Recognition</h1>
        
        <div class="model-info">
            <strong>ü§ñ Engine:</strong> Google Speech Recognition + Circular Buffer - Kh√¥ng b·ªã c·ª•t c√¢u
        </div>
        
        <div class="performance-info">
            <strong>‚ö° Features:</strong> Lookback 1s, 4s buffer, Real-time processing
        </div>
        
        <div class="status">
            <div id="connection-status" class="connection-status disconnected">
                üî¥ ƒêang k·∫øt n·ªëi...
            </div>
        </div>
        
        <div class="transcript-container">
            <h3>üìù Transcript Real-time:</h3>
            <div id="partial"></div>
            <div id="final"></div>
        </div>
        
        <div class="stats">
            <div class="stat-item">
                <div class="stat-value" id="packets-received">0</div>
                <div class="stat-label">Packets nh·∫≠n</div>
            </div>
            <div class="stat-item">
                <div class="stat-value" id="final-count">0</div>
                <div class="stat-label">Final results</div>
            </div>
            <div class="stat-item">
                <div class="stat-value" id="speech-engine">Circular Buffer</div>
                <div class="stat-label">Engine</div>
            </div>
        </div>
    </div>

    <script>
        var socket = io();
        var finalCount = 0;
        var packetsReceived = 0;

        // K·∫øt n·ªëi Socket.IO
        socket.on("connect", function() {
            document.getElementById("connection-status").className = "connection-status connected";
            document.getElementById("connection-status").innerHTML = "üü¢ ƒê√£ k·∫øt n·ªëi";
        });

        socket.on("disconnect", function() {
            document.getElementById("connection-status").className = "connection-status disconnected";
            document.getElementById("connection-status").innerHTML = "üî¥ M·∫•t k·∫øt n·ªëi";
        });

        // Nh·∫≠n final results t·ª´ Google Speech
        socket.on("final", function(msg) {
            finalCount++;
            let div = document.getElementById("final");
            let timestamp = new Date(msg.timestamp).toLocaleTimeString();
            
            div.innerHTML += `<div class="word">${msg.text}</div>`;
            document.getElementById("partial").innerHTML = "";
            document.getElementById("final-count").innerText = finalCount;
            
            // Auto-scroll xu·ªëng d∆∞·ªõi
            div.scrollTop = div.scrollHeight;
        });

        // C·∫≠p nh·∫≠t stats
        setInterval(function() {
            // C√≥ th·ªÉ th√™m c·∫≠p nh·∫≠t stats real-time ·ªü ƒë√¢y
        }, 1000);
    </script>
</body>
</html>"""
    
    with open("templates/index.html", "w", encoding="utf-8") as f:
        f.write(html_content)
    
    print("‚úÖ Templates ƒë√£ ƒë∆∞·ª£c t·∫°o")

def check_dependencies():
    """Ki·ªÉm tra dependencies c·∫ßn thi·∫øt"""
    return check_audio_dependencies()

if __name__ == "__main__":
    print("üöÄ Kh·ªüi ƒë·ªông ESP32 + INMP441 Real-time Streaming Server v·ªõi Circular Buffer...")
    print(f"üì° Flask server: http://localhost:{FLASK_PORT}")
    print(f"üéß UDP server: {HOST}:{UDP_PORT}")
    print(f"üåê Speech engine: Google Speech Recognition + Circular Buffer")
    print(f"üìä Buffer: {CIRCULAR_BUFFER_SIZE//1000}KB, Lookback: {LOOKBACK_SIZE//1000}KB")
    print("=" * 50)
    print("üèóÔ∏è MODULAR ARCHITECTURE (audio_utils):")
    print(f"   ‚Ä¢ audio_processing: Improved audio preprocessing")
    print(f"   ‚Ä¢ speech_recognition: Google Speech API integration")
    print(f"   ‚Ä¢ dependencies: Library dependency checker")
    print("=" * 50)
    print("üÜï C·∫¢I TI·∫æN SPEECH DETECTION:")
    print(f"   ‚Ä¢ MIN_SPEECH_RMS: {MIN_SPEECH_RMS} (gi·∫£m t·ª´ 500)")
    print(f"   ‚Ä¢ MIN_SILENCE_DURATION: {MIN_SILENCE_DURATION}s (gi·∫£m t·ª´ 1.0s)")
    print(f"   ‚Ä¢ MIN_AMPLITUDE_THRESHOLD: {MIN_AMPLITUDE_THRESHOLD}")
    print(f"   ‚Ä¢ SILENCE_RMS_THRESHOLD: {SILENCE_RMS_THRESHOLD}")
    print(f"   ‚Ä¢ SILENCE_AMPLITUDE_THRESHOLD: {SILENCE_AMPLITUDE_THRESHOLD}")
    print(f"   ‚Ä¢ MAX_RECORDING_DURATION: {MAX_RECORDING_DURATION}s (g·ª≠i ngay khi ƒë·∫°t max)")
    print(f"   ‚Ä¢ MIN_API_CALL_DELAY: {MIN_API_CALL_DELAY}s (delay gi·ªØa c√°c l·∫ßn g·ªçi API)")
    print(f"   ‚Ä¢ Adaptive threshold: T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh d·ª±a tr√™n background noise")
    print(f"   ‚Ä¢ Enhanced preprocessing: Noise gate + Speech boost")
    print(f"   ‚Ä¢ Multiple detection conditions: RMS + Amplitude + Sample strength")
    print(f"   ‚Ä¢ Improved silence detection: R√µ r√†ng ph√¢n bi·ªát speech/silence")
    print(f"   ‚Ä¢ Auto-send: G·ª≠i ngay khi ƒë·∫°t {MAX_RECORDING_DURATION}s ho·∫∑c silence {MIN_SILENCE_DURATION}s")
    print(f"   ‚Ä¢ Anti-spam: Delay {MIN_API_CALL_DELAY}s gi·ªØa c√°c l·∫ßn g·ªçi API")
    print("=" * 50)
    print("üîß STABILITY IMPROVEMENTS:")
    print(f"   ‚Ä¢ Queue empty protection: Ki·ªÉm tra tr∆∞·ªõc khi pop")
    print(f"   ‚Ä¢ Graceful shutdown: Signal handlers + shutdown events")
    print(f"   ‚Ä¢ Thread safety: Timeout + shutdown checks")
    print(f"   ‚Ä¢ Error handling: Spam log prevention")
    print("=" * 50)
    print("üìù TRANSCRIPT LOGGING:")
    print(f"   ‚Ä¢ Auto-save: M·ªói l·∫ßn detect ƒë∆∞·ª£c ch·ªØ s·∫Ω ghi ra file")
    print(f"   ‚Ä¢ Simple format: Ch·ªâ text, kh√¥ng c√≥ log ph·ª©c t·∫°p")
    print(f"   ‚Ä¢ File location: transcripts/live_transcript.txt")
    print(f"   ‚Ä¢ API endpoint: /transcript-stats ƒë·ªÉ xem th·ªëng k√™")
    print("=" * 50)
    print("üßπ BUFFER MANAGEMENT:")
    print(f"   ‚Ä¢ Auto-clear: X√≥a buffer sau m·ªói l·∫ßn x·ª≠ l√Ω API")
    print(f"   ‚Ä¢ Prevent duplicate: Tr√°nh l·∫∑p l·∫°i audio ƒë√£ x·ª≠ l√Ω")
    print(f"   ‚Ä¢ Fresh start: M·ªói l·∫ßn x·ª≠ l√Ω b·∫Øt ƒë·∫ßu v·ªõi buffer tr·ªëng")
    print(f"   ‚Ä¢ All cases: X√≥a buffer cho m·ªçi tr∆∞·ªùng h·ª£p (success/fail)")
    print("=" * 50)
    
    # Ki·ªÉm tra dependencies
    if not check_dependencies():
        print("‚ùå Thi·∫øu dependencies, vui l√≤ng c√†i ƒë·∫∑t tr∆∞·ªõc")
        exit(1)
    
    # Kh·ªüi t·∫°o TranscriptLogger
    transcript_logger = TranscriptLogger(output_dir="transcripts", filename="live_transcript.txt")
    print(f"üìù Transcript Logger: {transcript_logger.get_filepath()}")
    
    # T·∫°o templates
    create_templates()
    
    # Kh·ªüi ƒë·ªông c√°c threads
    udp_thread = threading.Thread(target=udp_listener, daemon=True)
    asr_thread = threading.Thread(target=asr_worker, daemon=True)
    
    print("üîÑ ƒêang kh·ªüi ƒë·ªông UDP Listener thread...")
    udp_thread.start()
    print("‚úÖ UDP Listener thread ƒë√£ kh·ªüi ƒë·ªông")
    
    print("üîÑ ƒêang kh·ªüi ƒë·ªông ASR Worker thread...")
    asr_thread.start()
    print("‚úÖ ASR Worker thread ƒë√£ kh·ªüi ƒë·ªông v·ªõi Circular Buffer")
    
    # Ki·ªÉm tra thread c√≥ alive kh√¥ng
    time.sleep(2)
    if asr_thread.is_alive():
        print("‚úÖ ASR Worker thread ƒëang ch·∫°y b√¨nh th∆∞·ªùng")
    else:
        print("‚ùå ASR Worker thread ƒë√£ d·ª´ng!")
    
    # Ch·∫°y Flask-SocketIO server
    print("üöÄ Kh·ªüi ƒë·ªông Flask-SocketIO server...")
    try:
        socketio.run(app, host="0.0.0.0", port=FLASK_PORT, debug=False)
    except KeyboardInterrupt:
        print("\nüõë Nh·∫≠n Ctrl+C, ƒëang d·ª´ng server...")
        shutdown_event.set()
        time.sleep(2)
        print("‚úÖ Server ƒë√£ d·ª´ng an to√†n")
    except Exception as e:
        print(f"‚ùå L·ªói Flask server: {e}")
        shutdown_event.set()
        time.sleep(2)
        print("‚úÖ Server ƒë√£ d·ª´ng an to√†n") 